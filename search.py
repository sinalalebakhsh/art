# search.py
import time
import random
import json
import os
import re
from urllib.parse import quote_plus, urlparse
import http.client
import ssl

try:
    import requests
    from bs4 import BeautifulSoup
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    print("âš ï¸  Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ requests Ù†ØµØ¨ Ù†ÛŒØ³Øª. Ù„Ø·ÙØ§ pip install requests Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.")

class SafeWebSearcher:
    def __init__(self):
        self.search_results = {}
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0'
        ]
        
        # Ù„ÛŒØ³Øª Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†
        self.search_engines = [
            'https://www.bing.com/search?q=',
            'https://search.yahoo.com/search?p=',
            'https://duckduckgo.com/html/?q=',
            'https://www.ask.com/web?q=',
            'https://www.baidu.com/s?wd='
        ]
        
    def is_available(self):
        return REQUESTS_AVAILABLE
    
    def get_random_delay(self):
        return random.uniform(2.0, 4.0)
    
    def get_random_user_agent(self):
        return random.choice(self.user_agents)
    
    def create_custom_ssl_context(self):
        """Ø§ÛŒØ¬Ø§Ø¯ SSL context Ø¨Ø±Ø§ÛŒ Ø¯ÙˆØ± Ø²Ø¯Ù† Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§"""
        context = ssl.create_default_context()
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE
        return context
    
    def manual_http_request(self, url):
        """Ø¯Ø±Ø®ÙˆØ§Ø³Øª HTTP Ø¯Ø³ØªÛŒ Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ§Ù‚Ø¹ÛŒ Ú©Ù‡ requests Ú©Ø§Ø± Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯"""
        try:
            parsed_url = urlparse(url)
            host = parsed_url.netloc
            path = parsed_url.path
            if parsed_url.query:
                path += '?' + parsed_url.query
            
            context = self.create_custom_ssl_context()
            
            if url.startswith('https'):
                conn = http.client.HTTPSConnection(host, context=context)
            else:
                conn = http.client.HTTPConnection(host)
            
            headers = {
                'User-Agent': self.get_random_user_agent(),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Connection': 'keep-alive',
            }
            
            conn.request("GET", path, headers=headers)
            response = conn.getresponse()
            
            if response.status == 200:
                data = response.read().decode('utf-8')
                conn.close()
                return data
            
            conn.close()
            return None
            
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¯Ø³ØªÛŒ: {e}")
            return None
    
    def safe_request(self, url):
        """Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§ÛŒÙ…Ù† Ø¨Ø§ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""
        if not REQUESTS_AVAILABLE:
            return None
        
        time.sleep(self.get_random_delay())
        
        # Ø±ÙˆØ´ 1: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² requests
        try:
            headers = {
                'User-Agent': self.get_random_user_agent(),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            response = requests.get(url, headers=headers, timeout=10, verify=False)
            if response.status_code == 200:
                return response.text
                
        except requests.exceptions.RequestException:
            # Ø±ÙˆØ´ 2: Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¯Ø³ØªÛŒ Ø§Ú¯Ø± requests Ú©Ø§Ø± Ù†Ú©Ø±Ø¯
            return self.manual_http_request(url)
        
        return None
    
    def search_with_bing(self, query):
        """Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ Ø¨ÛŒÙ†Ú¯ Ú©Ù‡ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ú©Ù…ØªØ±ÛŒ Ø¯Ø§Ø±Ø¯"""
        search_url = f"https://www.bing.com/search?q={quote_plus(query)}"
        html = self.safe_request(search_url)
        
        if html:
            try:
                soup = BeautifulSoup(html, 'html.parser')
                results = []
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ØªØ§ÛŒØ¬ Ø§Ø² Ø¨ÛŒÙ†Ú¯
                for result in soup.find_all('li', class_='b_algo'):
                    title_elem = result.find('h2')
                    link_elem = result.find('a')
                    desc_elem = result.find('p')
                    
                    if title_elem and link_elem:
                        results.append({
                            'title': title_elem.get_text().strip(),
                            'url': link_elem.get('href', ''),
                            'description': desc_elem.get_text().strip() if desc_elem else ''
                        })
                
                return results
            except:
                pass
        
        return None
    
    def search_with_duckduckgo(self, query):
        """Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ Ø¯Ø§Ú©â€ŒØ¯Ø§Ú©â€ŒÚ¯Ùˆ"""
        search_url = f"https://html.duckduckgo.com/html/?q={quote_plus(query)}"
        html = self.safe_request(search_url)
        
        if html:
            try:
                soup = BeautifulSoup(html, 'html.parser')
                results = []
                
                for result in soup.find_all('div', class_='result'):
                    title_elem = result.find('a', class_='result__a')
                    link_elem = result.find('a', class_='result__a')
                    desc_elem = result.find('a', class_='result__snippet')
                    
                    if title_elem and link_elem:
                        results.append({
                            'title': title_elem.get_text().strip(),
                            'url': link_elem.get('href', ''),
                            'description': desc_elem.get_text().strip() if desc_elem else ''
                        })
                
                return results
            except:
                pass
        
        return None
    
    def search_query(self, query, max_results=5):
        """Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ§Ù‚Ø¹ÛŒ"""
        if not REQUESTS_AVAILABLE:
            return self.get_mock_results(query)
        
        print(f"ðŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ: {query}")
        
        # Ø§ÙˆÙ„ Ø¨Ø§ Ø¨ÛŒÙ†Ú¯ Ø§Ù…ØªØ­Ø§Ù† Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        results = self.search_with_bing(query)
        
        # Ø§Ú¯Ø± Ø¨ÛŒÙ†Ú¯ Ø¬ÙˆØ§Ø¨ Ù†Ø¯Ø§Ø¯ØŒ Ø¨Ø§ Ø¯Ø§Ú©â€ŒØ¯Ø§Ú©â€ŒÚ¯Ùˆ Ø§Ù…ØªØ­Ø§Ù† Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        if not results:
            results = self.search_with_duckduckgo(query)
        
        # Ø§Ú¯Ø± Ø¨Ø§Ø²Ù‡Ù… Ø¬ÙˆØ§Ø¨ Ù†Ø¯Ø§Ø¯ØŒ Ø§Ø² Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        if not results:
            results = self.try_alternative_search_engines(query)
        
        if results:
            return results[:max_results]
        
        return self.get_mock_results(query)
    
    def try_alternative_search_engines(self, query):
        """Ø§Ù…ØªØ­Ø§Ù† Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†"""
        for engine_url in self.search_engines:
            try:
                search_url = engine_url + quote_plus(query)
                html = self.safe_request(search_url)
                
                if html:
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¹Ù…ÙˆÙ…ÛŒ
                    links = soup.find_all('a', href=True)
                    results = []
                    
                    for link in links:
                        href = link.get('href', '')
                        text = link.get_text().strip()
                        
                        # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±
                        if (href.startswith('http') and 
                            not href.startswith('http://webcache.googleusercontent.com') and
                            not href.startswith('http://www.google.com') and
                            len(text) > 10):
                            
                            results.append({
                                'title': text[:100],
                                'url': href,
                                'description': ''
                            })
                    
                    if results:
                        return results[:5]
                        
            except:
                continue
        
        return None
    
    def get_mock_results(self, query):
        """Ù†ØªØ§ÛŒØ¬ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ³Øª"""
        return [
            {
                'title': f'Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ: {query}',
                'url': 'https://www.example.com/search?q=' + quote_plus(query),
                'description': 'Ø§ÛŒÙ† ÛŒÚ© Ù†ØªÛŒØ¬Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Øª. Ø¨Ø±Ø§ÛŒ Ù†ØªØ§ÛŒØ¬ ÙˆØ§Ù‚Ø¹ÛŒ Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ requests Ù†ØµØ¨ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯.'
            },
            {
                'title': 'Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ù†ØµØ¨ requests',
                'url': 'https://pypi.org/project/requests/',
                'description': 'Ø¯Ø³ØªÙˆØ± Ù†ØµØ¨: pip install requests'
            }
        ]
    
    def save_to_dictionary(self, query, results):
        """Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬"""
        if query not in self.search_results:
            self.search_results[query] = []
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† timestamp Ø¨Ù‡ Ù‡Ø± Ù†ØªÛŒØ¬Ù‡
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        for result in results:
            result['search_time'] = timestamp
            result['query'] = query
        
        self.search_results[query].extend(results)
        self.save_to_file()
        
        print(f"âœ… {len(results)} Ù†ØªÛŒØ¬Ù‡ Ø¨Ø±Ø§ÛŒ '{query}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        return len(results)
    
    def save_to_file(self, filename="search_results.json"):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.search_results, f, ensure_ascii=False, indent=2)
            print(f"ðŸ’¾ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± {filename} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„: {e}")
    
    def load_from_file(self, filename="search_results.json"):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ø² ÙØ§ÛŒÙ„"""
        try:
            if os.path.exists(filename):
                with open(filename, 'r', encoding='utf-8') as f:
                    self.search_results = json.load(f)
                print(f"ðŸ“‚ Ù†ØªØ§ÛŒØ¬ Ø§Ø² {filename} Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
                return True
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„: {e}")
        return False
    
    def get_results(self, query=None):
        if query:
            return self.search_results.get(query, [])
        return self.search_results
    
    def clear_results(self):
        self.search_results = {}
        print("ðŸ—‘ï¸ Ù‡Ù…Ù‡ Ù†ØªØ§ÛŒØ¬ Ù¾Ø§Ú© Ø´Ø¯Ù†Ø¯")

# # ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ
# def search_and_save(query, max_results=3):
#     searcher = SafeWebSearcher()
#     searcher.load_from_file()
    
#     results = searcher.search_query(query, max_results)
    
#     if results:
#         searcher.save_to_dictionary(query, results)
#         return results
    
#     return []

# ØªØ³Øª ÙˆØ§Ù‚Ø¹ÛŒ
if __name__ == "__main__":
    print("ðŸ§ª ØªØ³Øª Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ§Ù‚Ø¹ÛŒ...")
    
    searcher = SafeWebSearcher()
    
    # ØªØ³Øª Ú†Ù†Ø¯ Ú©ÙˆØ¦Ø±ÛŒ
    test_queries = ["Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù†ÙˆÛŒØ³ÛŒ", "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ", "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†"]
    
    for query in test_queries:
        print(f"\nðŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ: {query}")
        results = searcher.search_query(query, 2)
        
        if results:
            searcher.save_to_dictionary(query, results)
            for i, result in enumerate(results, 1):
                print(f"   {i}. {result['title']}")
                print(f"      {result['url']}")
        else:
            print("   âŒ Ù‡ÛŒÚ† Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯")
        
        time.sleep(3)
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
    searcher.save_to_file()
    print(f"\nâœ… ØªØ³Øª Ú©Ø§Ù…Ù„ Ø´Ø¯. Ù†ØªØ§ÛŒØ¬ Ø¯Ø± search_results.json Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")

# Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ÛŒ search.py Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø§ÛŒØ¯ Ø¨Ø§Ø´Ø¯:
def search_and_save(query, max_results=3):
    searcher = SafeWebSearcher()
    searcher.load_from_file()
    
    results = searcher.search_query(query, max_results)
    
    if results:
        searcher.save_to_dictionary(query, results)
        return results
    
    return []

# # search.py
# import time
# import random
# import requests
# from bs4 import BeautifulSoup
# import json
# from urllib.parse import quote_plus
# import os

# class SafeWebSearcher:
#     def __init__(self):
#         self.search_results = {}
#         self.user_agents = [
#             'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
#             'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
#             'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
#             'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15'
#         ]
#         self.search_engines = [
#             'https://www.google.com/search?q=',
#             'https://www.bing.com/search?q=',
#             'https://search.yahoo.com/search?p=',
#             'https://duckduckgo.com/html/?q='
#         ]
        
#     def get_random_delay(self):
#         """ØªØ£Ø®ÛŒØ± ØªØµØ§Ø¯ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¨Ù„Ø§Ú© Ø´Ø¯Ù†"""
#         return random.uniform(2.0, 5.0)
    
#     def get_random_user_agent(self):
#         """User-Agent ØªØµØ§Ø¯ÙÛŒ"""
#         return random.choice(self.user_agents)
    
#     def rotate_search_engine(self, query, engine_index=0):
#         """Ú†Ø±Ø®Ø´ Ø¨ÛŒÙ† Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ"""
#         if engine_index >= len(self.search_engines):
#             return None
        
#         search_url = self.search_engines[engine_index] + quote_plus(query)
#         return search_url
    
#     def safe_request(self, url, max_retries=3):
#         """Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§ÛŒÙ…Ù† Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§"""
#         headers = {
#             'User-Agent': self.get_random_user_agent(),
#             'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
#             'Accept-Language': 'en-US,en;q=0.5',
#             'Accept-Encoding': 'gzip, deflate',
#             'Connection': 'keep-alive',
#             'Upgrade-Insecure-Requests': '1',
#         }
        
#         for attempt in range(max_retries):
#             try:
#                 time.sleep(self.get_random_delay())
                
#                 response = requests.get(
#                     url, 
#                     headers=headers, 
#                     timeout=10,
#                     allow_redirects=True
#                 )
                
#                 if response.status_code == 200:
#                     return response
#                 elif response.status_code == 429:  # Too Many Requests
#                     print(f"Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø²ÛŒØ§Ø¯! Ù…Ù†ØªØ¸Ø± Ù…ÛŒâ€ŒÙ…Ø§Ù†Ù…... (ØªÙ„Ø§Ø´ {attempt + 1})")
#                     time.sleep(10)
#                 else:
#                     print(f"Ø®Ø·Ø§ÛŒ HTTP {response.status_code}")
                    
#             except requests.exceptions.RequestException as e:
#                 print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„: {e}")
#                 time.sleep(5)
        
#         return None
    
#     def extract_results(self, html, search_engine):
#         """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ØªØ§ÛŒØ¬ Ø§Ø² HTML"""
#         soup = BeautifulSoup(html, 'html.parser')
#         results = []
        
#         try:
#             if 'google' in search_engine:
#                 # Ø¨Ø±Ø§ÛŒ Ú¯ÙˆÚ¯Ù„
#                 for g in soup.find_all('div', class_='tF2Cxc'):
#                     title_elem = g.find('h3')
#                     link_elem = g.find('a')
#                     desc_elem = g.find('span', class_='aCOpRe')
                    
#                     if title_elem and link_elem:
#                         results.append({
#                             'title': title_elem.get_text(),
#                             'url': link_elem.get('href'),
#                             'description': desc_elem.get_text() if desc_elem else ''
#                         })
            
#             elif 'bing' in search_engine:
#                 # Ø¨Ø±Ø§ÛŒ Ø¨ÛŒÙ†Ú¯
#                 for result in soup.find_all('li', class_='b_algo'):
#                     title_elem = result.find('h2')
#                     link_elem = result.find('a')
#                     desc_elem = result.find('p')
                    
#                     if title_elem and link_elem:
#                         results.append({
#                             'title': title_elem.get_text(),
#                             'url': link_elem.get('href'),
#                             'description': desc_elem.get_text() if desc_elem else ''
#                         })
            
#             elif 'yahoo' in search_engine or 'duckduckgo' in search_engine:
#                 # Ø¨Ø±Ø§ÛŒ ÛŒØ§Ù‡Ùˆ Ùˆ Ø¯Ø§Ú©â€ŒØ¯Ø§Ú©â€ŒÚ¯Ùˆ
#                 for result in soup.find_all('div', class_='compTitle'):
#                     title_elem = result.find('h3')
#                     link_elem = result.find('a')
                    
#                     if title_elem and link_elem:
#                         results.append({
#                             'title': title_elem.get_text(),
#                             'url': link_elem.get('href'),
#                             'description': ''
#                         })
                        
#         except Exception as e:
#             print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ØªØ§ÛŒØ¬: {e}")
        
#         return results
    
#     def search_query(self, query, max_results=10):
#         """Ø¬Ø³ØªØ¬ÙˆÛŒ ÛŒÚ© Ú©ÙˆØ¦Ø±ÛŒ"""
#         print(f"Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ: {query}")
        
#         all_results = []
#         engine_index = 0
        
#         while len(all_results) < max_results and engine_index < len(self.search_engines):
#             search_url = self.rotate_search_engine(query, engine_index)
#             if not search_url:
#                 break
            
#             print(f"Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬Ùˆ: {self.search_engines[engine_index].split('/')[2]}")
            
#             response = self.safe_request(search_url)
#             if response:
#                 results = self.extract_results(response.text, self.search_engines[engine_index])
#                 all_results.extend(results[:max_results - len(all_results)])
            
#             engine_index += 1
#             time.sleep(self.get_random_delay())
        
#         return all_results[:max_results]
    
#     def save_to_dictionary(self, query, results):
#         """Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ"""
#         if query not in self.search_results:
#             self.search_results[query] = []
        
#         self.search_results[query].extend(results)
        
#         # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ JSON Ø¨Ø±Ø§ÛŒ backup
#         self.save_to_file()
        
#         return len(results)
    
#     def save_to_file(self, filename="search_results.json"):
#         """Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„"""
#         try:
#             with open(filename, 'w', encoding='utf-8') as f:
#                 json.dump(self.search_results, f, ensure_ascii=False, indent=2)
#             print(f"Ù†ØªØ§ÛŒØ¬ Ø¯Ø± {filename} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
#         except Exception as e:
#             print(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„: {e}")
    
#     def load_from_file(self, filename="search_results.json"):
#         """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø§Ø² ÙØ§ÛŒÙ„"""
#         try:
#             if os.path.exists(filename):
#                 with open(filename, 'r', encoding='utf-8') as f:
#                     self.search_results = json.load(f)
#                 print(f"Ù†ØªØ§ÛŒØ¬ Ø§Ø² {filename} Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
#                 return True
#         except Exception as e:
#             print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„: {e}")
#         return False
    
#     def get_results(self, query=None):
#         """Ø¯Ø±ÛŒØ§ÙØª Ù†ØªØ§ÛŒØ¬"""
#         if query:
#             return self.search_results.get(query, [])
#         return self.search_results
    
#     def clear_results(self):
#         """Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ù†ØªØ§ÛŒØ¬"""
#         self.search_results = {}
#         print("Ù‡Ù…Ù‡ Ù†ØªØ§ÛŒØ¬ Ù¾Ø§Ú© Ø´Ø¯Ù†Ø¯")

# # ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø®Ø§Ø±Ø¬
# def search_and_save(query, max_results=5):
#     """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ùˆ Ø°Ø®ÛŒØ±Ù‡"""
#     searcher = SafeWebSearcher()
    
#     # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬ Ù‚Ø¨Ù„ÛŒ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
#     searcher.load_from_file()
    
#     # Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©ÙˆØ¦Ø±ÛŒ Ø¬Ø¯ÛŒØ¯
#     results = searcher.search_query(query, max_results)
    
#     if results:
#         count = searcher.save_to_dictionary(query, results)
#         print(f"{count} Ù†ØªÛŒØ¬Ù‡ Ø¨Ø±Ø§ÛŒ '{query}' Ù¾ÛŒØ¯Ø§ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
#         return results
#     else:
#         print("Ù‡ÛŒÚ† Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯")
#         return []

# # Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡
# if __name__ == "__main__":
#     # ØªØ³Øª Ø¹Ù…Ù„Ú©Ø±Ø¯
#     queries = ["Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù†ÙˆÛŒØ³ÛŒ", "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ", "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†"]
    
#     searcher = SafeWebSearcher()
    
#     for query in queries:
#         results = searcher.search_query(query, max_results=3)
#         if results:
#             searcher.save_to_dictionary(query, results)
#             print(f"Ù†ØªØ§ÛŒØ¬ Ø¨Ø±Ø§ÛŒ '{query}':")
#             for i, result in enumerate(results, 1):
#                 print(f"{i}. {result['title']}")
#                 print(f"   URL: {result['url']}")
#                 print()
        
#         time.sleep(3)  # ØªØ£Ø®ÛŒØ± Ø¨ÛŒÙ† Ø¬Ø³ØªØ¬ÙˆÙ‡Ø§
    
#     # Ø°Ø®ÛŒØ±Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
#     searcher.save_to_file()
#     print("Ù‡Ù…Ù‡ Ù†ØªØ§ÛŒØ¬ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")